{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-08T12:31:29.096584Z",
     "start_time": "2019-08-08T12:31:27.389793Z"
    }
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import requests\n",
    "\n",
    "import re\n",
    "\n",
    "from pandas import DataFrame\n",
    "\n",
    "import datetime\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "\n",
    "con = sqlite3.connect(\"news.db\") #test.db생성\n",
    "cur = con.cursor()\n",
    "\n",
    "cur.executescript(\"\"\"\n",
    "        DROP TABLE IF EXISTS table1;\n",
    "        CREATE TABLE table1(\n",
    "        id INTEGER PRIMARY KEY AUTOINCREMENT NOT NULL,\n",
    "        link TEXT NOT NULL\n",
    "        );\n",
    "\n",
    "        DROP TABLE IF EXISTS table2;\n",
    "        CREATE TABLE table2(\n",
    "        id INTEGER PRIMARY KEY AUTOINCREMENT NOT NULL,\n",
    "        date TEXT NOT NULL,\n",
    "        title TEXT NOT NULL,\n",
    "        content TEXT NOT NULL,\n",
    "        link TEXT NOT NULL\n",
    "        );\n",
    "\"\"\")\n",
    "\n",
    "con.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-08T12:31:29.103530Z",
     "start_time": "2019-08-08T12:31:29.097577Z"
    }
   },
   "outputs": [],
   "source": [
    "def download(method,url,param=None,data=None,timeout=1,maxretries=3):\n",
    "    try:\n",
    "        resp=requests.request(method,url,params=param,data=data)\n",
    "        resp.raise_for_status()\n",
    "    except requests.exceptions.HTTPError as e:\n",
    "        if 500<=e.response.status_code<600 and maxretries>0 :\n",
    "            print(maxretries)\n",
    "            time.sleep(timeout)\n",
    "            download(method,url,param,data,timeout,maxretries-1)\n",
    "            print(\"재시도\")\n",
    "        else:\n",
    "            print(e.reponse.status_code)\n",
    "            print(e.response.reason)\n",
    "    return resp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-08T12:31:29.121481Z",
     "start_time": "2019-08-08T12:31:29.105525Z"
    }
   },
   "outputs": [],
   "source": [
    "def news_1page(p_id=264,dates=20190730,pages_num=1):\n",
    "    url=\"https://news.naver.com/main/list.nhn?mode=LS2D\"\n",
    "    param={\n",
    "        'sid2':p_id,\n",
    "        'sid1':100,\n",
    "        'mid':'shm',\n",
    "        'date':dates,\n",
    "        'page':pages_num,\n",
    "    }\n",
    "    html=download('get',url,param=param)\n",
    "    dom=BeautifulSoup(html.text,'lxml')\n",
    "\n",
    "    tk=dom.find_all('td',{\"class\":'content'})[0] .find_all({'a':'href'})  #링크,날짜,페이지 들어간거 찾고 a 그래도 링크,페이지,날짜\n",
    "\n",
    "    #이미지 중복 링크 제거용도 이미지 없는거에 대한 대비책\n",
    "    tktk=[]\n",
    "    for i in range(len(tk)):\n",
    "        try:\n",
    "            len(tk[i].img) #이미지를 뽑아냈을때 길이가 있다는거는 이미지 관련 부분\n",
    "        except:\n",
    "            tktk.append(list(tk[i].attrs.values())) #이미지 아닌 부분 출력\n",
    "            \n",
    "\n",
    "    tktk=[y for x in tktk for y in x] #멀티 리스트 되있는거를 풀어줄때 사용 -> 링크, 페이지랑, 날짜\n",
    "\n",
    "    #필요없는 날짜부분 제거 길이 방식으로 그냥 제거 날짜부분 길이가 1임을 이용했음\n",
    "    tktk2=[]\n",
    "    for i in range(len(tktk)):\n",
    "        if len(tktk[i])>2:\n",
    "            tktk2.append(tktk[i])\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "    #링크부분 뽑아내기\n",
    "    links=[]\n",
    "    for i in range(len(tktk2)):\n",
    "        if tktk2[i].find('https')>(-1): #find를 이용해서 https가 없으면은 -1 있으면 -1 초과의 값 0~\n",
    "            links.append(tktk2[i])\n",
    "        else:\n",
    "            pass\n",
    "    links2=DataFrame(links,columns=['links'])\n",
    "\n",
    "    #1페이지, 11페이지 21.... 페이지만 뽑아내기\n",
    "    pages=[]\n",
    "    for i in range(len(tktk2)):\n",
    "        if tktk2[i].find('page')>(-1): #find 방식으로 페이지 뽑아내기\n",
    "            pages.append(tktk2[i][-3:]) #여기서 page=11 page=9\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "    pages=[i.replace('e=','') for i in pages] #replace를 이용해서 \n",
    "    pages=[i.replace('=','') for i in pages]\n",
    "    pages=[int(s) for s in pages] #int 바꾸려고 해당 부분 사용\n",
    "\n",
    "    pages2=[] #페이지 중복 제거 #11 -> 10 12 13 14 ....\n",
    "    for i in range(len(pages)):\n",
    "        if pages[i]>pages_num: #제일 위에 페이지 넘버 가지고 와서 그 숫자보다 큰 것만 추가\n",
    "            pages2.append(pages[i])\n",
    "        else:\n",
    "            pass\n",
    "    pages=[str(s) for s in pages2] #그냥 숫자값으로 들어가면 페이지 부분에서 오류 생겨서 str로 변경\n",
    "    \n",
    "    return links,pages\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-08T12:31:29.137467Z",
     "start_time": "2019-08-08T12:31:29.124472Z"
    }
   },
   "outputs": [],
   "source": [
    "def news_other_page(p_id=264,dates=20190730,pages_num=2):\n",
    "    url=\"https://news.naver.com/main/list.nhn?mode=LS2D\"\n",
    "    param={\n",
    "        'sid2':p_id,\n",
    "        'sid1':100,\n",
    "        'mid':'shm',\n",
    "        'date':dates,\n",
    "        'page':pages_num,\n",
    "    }\n",
    "    html=download('get',url,param=param)\n",
    "    dom=BeautifulSoup(html.text,'lxml')\n",
    "    tk=dom.find_all('td',{\"class\":'content'})[0].find_all({'a':'href'})\n",
    "    \n",
    "    tktk=[]\n",
    "    for i in range(len(tk)):\n",
    "        try:\n",
    "            len(tk[i].img)\n",
    "        except:\n",
    "            tktk.append(list(tk[i].attrs.values()))\n",
    "\n",
    "    tktk=[y for x in tktk for y in x]\n",
    "\n",
    "    tktk2=[]\n",
    "    for i in range(len(tktk)):\n",
    "        if len(tktk[i])>2:\n",
    "            tktk2.append(tktk[i])\n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "    links=[]\n",
    "    for i in range(len(tktk2)):\n",
    "        if tktk2[i].find('https')>(-1):\n",
    "            links.append(tktk2[i])\n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "    return links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-08T12:31:29.158534Z",
     "start_time": "2019-08-08T12:31:29.138471Z"
    }
   },
   "outputs": [],
   "source": [
    "# page_total=[]\n",
    "# for i in range(10):\n",
    "#     links2,pages2=news_1page(p_id=265,dates=20190802,pages_num=(10*i)+1)\n",
    "#     page_total.append(pages2)\n",
    "# page_total=[y for x in page_total for y in x]\n",
    "\n",
    "# links_total,pages2=news_1page(p_id=265,dates=20190802,pages_num=1)\n",
    "# links_total=DataFrame(links_total,columns=['links'])\n",
    "\n",
    "# for i in range(2,len(page_total)+2):\n",
    "#     links=DataFrame(news_other_page(p_id=265,dates=20190802,pages_num=i),columns=['links'])\n",
    "#     links_total=pd.concat([links_total,links],axis=0)\n",
    "# links_total=links_total.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-08T12:31:29.185566Z",
     "start_time": "2019-08-08T12:31:29.181571Z"
    }
   },
   "outputs": [],
   "source": [
    "# a = datetime.datetime(2019,8,2)\n",
    "# links_real_total=DataFrame(columns=['links'])\n",
    "# s_id=264\n",
    "\n",
    "# for j in range(3):\n",
    "#     d_time=int((a - datetime.timedelta(days=j)).strftime('%Y%m%d'))\n",
    "#     page_total=[]\n",
    "    \n",
    "#     for i in range(10):\n",
    "#         links2,pages2=news_1page(p_id=s_id,dates=d_time,pages_num=(10*i)+1)\n",
    "#         page_total.append(pages2)\n",
    "#     page_total=[y for x in page_total for y in x]\n",
    "\n",
    "#     links_total,pages2=news_1page(p_id=s_id,dates=d_time,pages_num=1)\n",
    "#     links_total=DataFrame(links_total,columns=['links'])\n",
    "\n",
    "#     for k in range(2,len(page_total)+2):\n",
    "#         links=DataFrame(news_other_page(p_id=s_id,dates=d_time,pages_num=k),columns=['links'])\n",
    "#         links_total=pd.concat([links_total,links],axis=0)\n",
    "#         links_total=links_total.reset_index(drop=True)\n",
    "    \n",
    "#     links_real_total=pd.concat([links_real_total,links_total],axis=0)\n",
    "#     links_real_total=links_real_total.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-08T12:32:01.344320Z",
     "start_time": "2019-08-08T12:31:31.927346Z"
    }
   },
   "outputs": [],
   "source": [
    "a = datetime.datetime(2019,8,7) #원하는 날짜 지정\n",
    "links_real_total=DataFrame(columns=['links']) #링크 빈 data frame\n",
    "\n",
    "for m in range(264,270): #264~269 \n",
    "    for j in range(10): #원하는 날짜 주기 # 1넣으면 당일만 #3 넣으면은 8/2, 8/1 7/31 \n",
    "        d_time=int((a - datetime.timedelta(days=j)).strftime('%Y%m%d')) #기준 날짜에서 빼서 하는거 8/2 -> 8/1 \n",
    "        page_total=[] #빈 페이지 번호\n",
    "\n",
    "        #해당 카테코리 부분의 페이지 번호 전체 가져오기\n",
    "        for i in range(15): #여유있게 지정해둔 것\n",
    "            links2,pages2=news_1page(p_id=m,dates=d_time,pages_num=(10*i)+1) #1페이지 -> 11페이지 -> 21 -> 31 .....\n",
    "            page_total.append(pages2) #페이지가 동일하게 처리 되는 시점 부터는 최종 페이지까지만 나오게 됨\n",
    "        page_total=[y for x in page_total for y in x] #멀티 리스트 처리\n",
    "\n",
    "        links_total,pages2=news_1page(p_id=m,dates=d_time,pages_num=1) #링크 처음부터 넣는 부분\n",
    "        links_total=DataFrame(links_total,columns=['links']) #1페이지에 있는 링크 dataframe\n",
    " \n",
    "        #2페이지부터의 과정\n",
    "        for k in range(2,len(page_total)+2):\n",
    "            links=DataFrame(news_other_page(p_id=m,dates=d_time,pages_num=k),columns=['links']) #2페이지 짜리는 바로 링크만 있으니 바로 dataframe\n",
    "            links_total=pd.concat([links_total,links],axis=0) #concat 해서 dataframe 바로 밑으로 합치기 \n",
    "            links_total=links_total.reset_index(drop=True)\n",
    "\n",
    "        links_real_total=pd.concat([links_real_total,links_total],axis=0)\n",
    "        links_real_total=links_real_total.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-08T12:42:29.508538Z",
     "start_time": "2019-08-08T12:41:37.397280Z"
    }
   },
   "outputs": [],
   "source": [
    "#링크 단계에서 오류가 났을때 일단 집어 넣고 돌릴수 있도록 만들어 놓음\n",
    "for i in range(len(links_real_total)):\n",
    "    cur.execute(\"\"\"\n",
    "    INSERT INTO table1\n",
    "    (link)\n",
    "    VALUES(?)\n",
    "    \"\"\", [links_real_total['links'][i]])\n",
    "    con.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-08T09:28:37.799398Z",
     "start_time": "2019-08-08T09:28:12.107860Z"
    }
   },
   "outputs": [],
   "source": [
    "# link=[]\n",
    "# date=[]\n",
    "# title=[]\n",
    "# write=[]\n",
    "\n",
    "# # for i in range(len(links_real_total)):\n",
    "# for i in range(2600,2700):\n",
    "#     html=download('get',links_real_total['links'][i])\n",
    "#     dom=BeautifulSoup(html.text,'lxml')\n",
    "#     time.sleep(0.1)\n",
    "#     try:\n",
    "#         oo=dom.find('div',{'class':'sponsor'}).text #정치 기사가 아닌 텍스트 골라내기 용도\n",
    "#     except:\n",
    "#         pass\n",
    "#     else:\n",
    "#         link.append(links_real_total['links'][i])\n",
    "#         #date 입력되는 길이 이용해서 걸러내는 방법\n",
    "#         date.append(oo[dom.find('div',{'class':'sponsor'}).text.find('기사입력')+5:dom.find('div',{'class':'sponsor'}).text.find('기사입력')+25])\n",
    "\n",
    "#         #title 찾고 추가로 들어가있는 부분 대체하는 방법\n",
    "#         title.append(dom.find('title').text.replace(\" : 네이버 뉴스\",\"\"))\n",
    "\n",
    "#         #본문 일단 다 가져오는 방법 #함수 제거하는 것과 광고 부분을 길이를 이용해서 제거해버림\n",
    "#         o2=dom.find_all('div',{'id':'articleBodyContents'})[0].text\n",
    "#         write.append(o2[dom.find_all('div',{'id':'articleBodyContents'})[0].text.find('()')+3:dom.find_all('div',{'id':'articleBodyContents'})[0].text.find('▶')])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-08T13:12:11.462156Z",
     "start_time": "2019-08-08T12:43:32.447979Z"
    }
   },
   "outputs": [],
   "source": [
    "for i in range(len(links_real_total)):\n",
    "    html=download('get',links_real_total['links'][i])\n",
    "    dom=BeautifulSoup(html.text,'lxml')\n",
    "    time.sleep(0.2) #timeout error 방지용 최소마지노선\n",
    "    try:\n",
    "        oo=dom.find('div',{'class':'sponsor'}).text #정치 기사가 아닌 텍스트 골라내기 용도 #정치기사 아닌거 돌리면 무조건 에러남\n",
    "    except:\n",
    "        pass #정치기사 아닌건 db에 안 넣을 거임\n",
    "    else:\n",
    "        o2=dom.find_all('div',{'id':'articleBodyContents'})[0].text #기사 내용 걸러내기 1차, 밑에는 2차 걸러내기\n",
    "        content=o2[dom.find_all('div',{'id':'articleBodyContents'})[0].text.find('()')+3:dom.find_all('div',{'id':'articleBodyContents'})[0].text.find('▶')].replace('\\xa0','').replace(\"\\'\",'').replace('\\t','').replace('{}\\n\\n','')\n",
    "        if dom.find('title').text.replace(\" : 네이버 뉴스\",\"\").find('속보')>(-1) or len(content)<350: #속보인거랑 길이 350가 안되는 사진기사나 간략 기사는 걸러낼거임\n",
    "            pass\n",
    "        else:\n",
    "            date=oo[dom.find('div',{'class':'sponsor'}).text.find('기사입력')+5:dom.find('div',{'class':'sponsor'}).text.find('기사입력')+25].replace('\\n','').replace('\\r','')\n",
    "            #날짜 부분은 길이를 이용해서 추가하고 삐져나온 부분만 제거해주는 방식으로 함\n",
    "            \n",
    "            #주어진 table에다가 넣는 방식으로 하고 이후에 db에서 골라낸 뒤에 다시 집어넣는 방식을 사용하는 것을 생각하고 있음\n",
    "            cur.execute(\"\"\"\n",
    "            INSERT INTO table2\n",
    "            (date, title, content, link)\n",
    "            VALUES(?,?,?,?)\n",
    "            \"\"\", [date, dom.find('title').text.replace(\" : 네이버 뉴스\",\"\"), content, links_real_total['links'][i]] )\n",
    "            con.commit()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "하루치 돌아가는데 4900개 기준 28분"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-08T09:48:33.682686Z",
     "start_time": "2019-08-08T09:48:10.545355Z"
    }
   },
   "outputs": [],
   "source": [
    "# # for i in range(len(links_real_total)):\n",
    "# for i in range(2600,2700):\n",
    "#     html=download('get',links_real_total['links'][i])\n",
    "#     dom=BeautifulSoup(html.text,'lxml')\n",
    "#     time.sleep(0.1)\n",
    "#     try:\n",
    "#         oo=dom.find('div',{'class':'sponsor'}).text #정치 기사가 아닌 텍스트 골라내기 용도 #정치기사 아닌거 돌리면 무조건 에러남\n",
    "#     except:\n",
    "#         pass #정치기사 아닌건 db에 안 넣을음\n",
    "#     else:\n",
    "#         date=oo[dom.find('div',{'class':'sponsor'}).text.find('기사입력')+5:dom.find('div',{'class':'sponsor'}).text.find('기사입력')+25].replace('\\n','').replace('\\r','')\n",
    "        \n",
    "#         o2=dom.find_all('div',{'id':'articleBodyContents'})[0].text\n",
    "#         content=o2[dom.find_all('div',{'id':'articleBodyContents'})[0].text.find('()')+3:dom.find_all('div',{'id':'articleBodyContents'})[0].text.find('▶')].replace('\\xa0','').replace(\"\\'\",'').replace('\\t','')\n",
    "        \n",
    "#         cur.execute(\"\"\"\n",
    "#         INSERT INTO table1\n",
    "#         (date, title, content, link)\n",
    "#         VALUES(?,?,?,?)\n",
    "#         \"\"\", [date, dom.find('title').text.replace(\" : 네이버 뉴스\",\"\"), content, links_real_total['links'][i]] )\n",
    "#         con.commit()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
